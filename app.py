# app.py â€” å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ç‰ˆ CMMS ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆCSVå–è¾¼ã¤ãï¼‰
import os, io, textwrap, re
from datetime import date
import pandas as pd
import duckdb as ddb
import streamlit as st
import plotly.express as px
import numpy as np
from plotly.subplots import make_subplots
import plotly.graph_objects as go

st.set_page_config(page_title="CMMS ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ï¼ˆÎ²ï¼‰", layout="wide", page_icon="ğŸ› ï¸")

# ---- DB åˆæœŸåŒ– -----------------------------------------------------------
DB_PATH = os.environ.get("CMMS_DUCKDB_PATH", "app.duckdb")
os.makedirs(os.path.dirname(DB_PATH) or ".", exist_ok=True)
con = ddb.connect(DB_PATH)
con.execute("""
CREATE TABLE IF NOT EXISTS buildings(tenant TEXT, id TEXT, name TEXT, PRIMARY KEY(tenant,id));
CREATE TABLE IF NOT EXISTS locations(tenant TEXT, id TEXT, building_id TEXT, name TEXT, PRIMARY KEY(tenant,id));
CREATE TABLE IF NOT EXISTS floors(tenant TEXT, id TEXT, location_id TEXT, name TEXT, PRIMARY KEY(tenant,id));
CREATE TABLE IF NOT EXISTS rooms(tenant TEXT, id TEXT, floor_id TEXT, name TEXT, PRIMARY KEY(tenant,id));
CREATE TABLE IF NOT EXISTS devices(tenant TEXT, id TEXT, building_id TEXT, location_id TEXT, floor_id TEXT, room_id TEXT,
  name TEXT, category_l TEXT, category_m TEXT, category_s TEXT, symbol TEXT, cmms_url_rule TEXT,
  PRIMARY KEY(tenant,id));
CREATE TABLE IF NOT EXISTS targets(tenant TEXT, id TEXT, device_id TEXT, name TEXT, input_type TEXT,
  unit TEXT, lower DOUBLE, upper DOUBLE, ord INT, PRIMARY KEY(tenant,id));
CREATE TABLE IF NOT EXISTS schedules(tenant TEXT, id TEXT, job_id TEXT, name TEXT, freq TEXT, PRIMARY KEY(tenant,id));
CREATE TABLE IF NOT EXISTS schedule_targets(tenant TEXT, schedule_id TEXT, target_id TEXT,
  PRIMARY KEY(tenant,schedule_id,target_id));
CREATE TABLE IF NOT EXISTS schedule_dates(tenant TEXT, schedule_id TEXT, date DATE, status TEXT,
  done INT, total INT, done_at TIMESTAMP, PRIMARY KEY(tenant,schedule_id,date));
CREATE TABLE IF NOT EXISTS results(tenant TEXT, schedule_id TEXT, date DATE, target_id TEXT,
  value_num DOUBLE, value_text TEXT, value_bool BOOLEAN, judged TEXT,
  PRIMARY KEY(tenant,schedule_id,date,target_id));
CREATE TABLE IF NOT EXISTS issues(tenant TEXT, id TEXT, device_id TEXT, reported_on DATE, due_on DATE,
  status TEXT, severity TEXT, category TEXT, summary TEXT, cmms_url_rule TEXT, PRIMARY KEY(tenant,id));
CREATE TABLE IF NOT EXISTS documents(tenant TEXT, id TEXT, title TEXT, category TEXT, tags TEXT,
  url TEXT, version TEXT, ocr_status TEXT, ai_summary TEXT, ai_actions TEXT,
  uploaded_at TIMESTAMP DEFAULT now(), PRIMARY KEY(tenant,id));
CREATE TABLE IF NOT EXISTS document_bindings(tenant TEXT, doc_id TEXT, entity_type TEXT, entity_id TEXT,
  PRIMARY KEY(tenant,doc_id,entity_type,entity_id));
CREATE TABLE IF NOT EXISTS daily_kpis(tenant TEXT, date DATE, planned INT, done INT, overdue INT, findings INT,
  PRIMARY KEY(tenant,date));
""")

TENANT = st.session_state.setdefault("tenant", "demo")

# ---- å…±é€š: ç°¡æ˜“ã‚¢ãƒƒãƒ—ã‚µãƒ¼ãƒˆ ---------------------------------------------
def upsert_df(table: str, df: pd.DataFrame, pk: list[str]):
    if df.empty:
        return
    df = df.copy()
    df["tenant"] = TENANT

    # PK ã‚’æ­£è¦åŒ– & æ¬ æ/ç©ºã‚’é™¤å¤–
    for k in pk:
        df[k] = df[k].apply(_norm)
    mask_valid = df[pk].apply(lambda s: s.astype(str).str.len() > 0).all(axis=1)
    bad = len(df) - int(mask_valid.sum())
    if bad:
        st.warning(f"{table}: PKæ¬ æã§ {bad} è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—ï¼ˆç©º/NaN/Noneï¼‰")
    df = df[mask_valid]

    # PK é‡è¤‡ã‚’æŠ˜ã‚ŠãŸãŸã¿
    before = len(df)
    df = df.drop_duplicates(subset=pk, keep="last")
    dup = before - len(df)
    if dup:
        st.warning(f"{table}: PKé‡è¤‡ã§ {dup} è¡Œã‚’æŠ˜ã‚ŠãŸãŸã¿")

    cols = list(df.columns)
    for _, r in df.iterrows():
        where = " AND ".join([f"{k} = ?" for k in ["tenant", *pk]])
        con.execute(f"DELETE FROM {table} WHERE {where}", [TENANT, *[r[k] for k in pk]])
        con.execute(
            f"INSERT INTO {table}({','.join(cols)}) VALUES ({','.join(['?']*len(cols))})",
            r.tolist()
        )


# ---- ä¾¿åˆ©ï¼šéšå±¤ãƒ‘ã‚¹ã§ä¸€æ„ãª device_id ã‚’ä½œã‚‹ ----------------------------
# --- IDç”Ÿæˆãƒ˜ãƒ«ãƒ‘ï¼ˆæ¬ æ/ç©º/ã‚¹ãƒšãƒ¼ã‚¹/Noneã‚’å¸åï¼‰---
def _norm(x):
    if x is None or (isinstance(x, float) and pd.isna(x)):
        return ""
    s = str(x).strip()
    return "" if s.lower() in ("nan", "none") else s

def _dev_id(b, l, f, r, name):
    # ã™ã¹ã¦æ­£è¦åŒ–ã—ã¦ã‹ã‚‰é€£çµ
    return "|".join([_norm(b), _norm(l), _norm(f), _norm(r), _norm(name)])


# ---- å–è¾¼ï¼šãƒã‚¹ã‚¿ï¼ˆéšå±¤ + è¨­å‚™ + target + schedule ç´ä»˜ã‘ï¼‰ ------------
def import_master(df: pd.DataFrame):
    # â¶ åˆ—åæ•´å½¢
    df = df.copy()
    df.columns = [str(c).strip().lower()for c in df.columns]

    # â· ã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆæ—¥æœ¬èª/è‹±èªã©ã¡ã‚‰ã§ã‚‚ï¼‰
    alias = {
        "building_name": ["building","building_id","ç‰©ä»¶","ç‰©ä»¶å"],
        "location_name": ["location","æ£Ÿ","æ£Ÿå","ãƒ–ãƒ­ãƒƒã‚¯"],
        "floor_name":    ["floor","éš","ãƒ•ãƒ­ã‚¢"],
        "room_name":     ["room","éƒ¨å±‹","å®¤","åŒºç”»"],
        "device_name":   ["device","è¨­å‚™","è¨­å‚™å"],
        "target_id":     ["ç‚¹æ¤œé …ç›®id","target"],
        "target_name":   ["ç‚¹æ¤œé …ç›®å","target_label","é …ç›®å"],
        "target_type_id":["input_type","ç¨®åˆ¥","å…¥åŠ›å‹"],
        "schedule_id":   ["schedule","ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«id","ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«"],
        "unit":          ["å˜ä½"],
        "lower":         ["ä¸‹é™","min"],
        "upper":         ["ä¸Šé™","max"],
        "order_no":      ["ord","é †åº","order"],
    }
    for canon, alts in alias.items():
        if canon not in df.columns:
            for a in alts:
                if a in df.columns:
                    df.rename(columns={a: canon}, inplace=True)
                    break

    # â¸ å¿…é ˆåˆ—ãƒã‚§ãƒƒã‚¯ï¼ˆã“ã“ã§ã ã‘è½ã¨ã™ï¼‰
    need = ["building_name","location_name","room_name",
            "device_name","target_id","target_name","target_type_id","schedule_id"]
    miss = [c for c in need if c not in df.columns]
    if miss:
        st.error(f"å¿…é ˆåˆ—ãŒä¸è¶³: {miss}")
        st.write("å—ã‘å–ã£ãŸåˆ—:", list(df.columns))
        return

    # â¹ ä»»æ„åˆ—ã‚’è£œå®Œ
    defaults = {"floor_name":"", "unit":None, "lower":None, "upper":None, "order_no":0}
    for c, v in defaults.items():
        if c not in df.columns:
            df[c] = v

    # ä»¥é™ã¯æ—¢å­˜ãƒ­ã‚¸ãƒƒã‚¯ãã®ã¾ã¾ -----------------------------
    # devices ã®IDï¼ˆéšå±¤ãƒ‘ã‚¹ï¼‰
    def _dev_id(b,l,f,r,name): return f"{b}|{l}|{f}|{r}|{name}"

       # --- devicesï¼šå®‰å…¨ã«IDä½œæˆ â†’ é‡è¤‡/æ¬ æã‚’é™¤å»ã—ã¦UPSERT ---
    d = df[["building_name","location_name","floor_name","room_name","device_name"]].copy()
    d = d.rename(columns={"building_name":"building_id","location_name":"location_id",
                          "floor_name":"floor_id","room_name":"room_id","device_name":"name"})
    for c in ["building_id","location_id","floor_id","room_id","name"]:
        d[c] = d[c].apply(_norm)

    # è¨­å‚™åãŒç©ºã®è¡Œã¯é™¤å¤–
    d = d[d["name"].str.len() > 0]

    # IDä½œæˆ
    d["id"] = d.apply(lambda r: _dev_id(r["building_id"], r["location_id"],
                                        r["floor_id"], r["room_id"], r["name"]), axis=1)

    # IDé‡è¤‡ãŒã‚ã‚Œã°è­¦å‘Šã—ã¤ã¤ç•³ã‚€
    if d.duplicated(subset=["id"]).any():
        cnt = int(d.duplicated(subset=["id"], keep=False).sum())
        st.warning(f"devices: åŒä¸€IDãŒ {cnt} è¡Œè¦‹ã¤ã‹ã‚Šã¾ã—ãŸï¼ˆä¸€éƒ¨ã‚’æŠ˜ã‚ŠãŸãŸã¿ã¾ã™ï¼‰")
    d = d.drop_duplicates(subset=["id"], keep="last")

    d["category_l"]=d["category_m"]=d["category_s"]=d["symbol"]=d["cmms_url_rule"]=None
    upsert_df(
        "devices",
        d[["id","building_id","location_id","floor_id","room_id","name",
           "category_l","category_m","category_s","symbol","cmms_url_rule"]],
        ["id"]
    )

    # --- targetsï¼šdevice_id ã‚’å®‰å…¨ç”Ÿæˆã—ã¦UPSERT ---
    t = df[["building_name","location_name","floor_name","room_name","device_name",
            "target_id","target_name","target_type_id","unit","lower","upper","order_no"]].copy()

    for c in ["building_name","location_name","floor_name","room_name","device_name"]:
        t[c] = t[c].apply(_norm)

    t["device_id"] = t.apply(lambda r: _dev_id(r["building_name"], r["location_name"],
                                               r["floor_name"], r["room_name"], r["device_name"]), axis=1)
    t = t.rename(columns={"target_id":"id","target_name":"name",
                          "target_type_id":"input_type","order_no":"ord"})
    t = t.drop_duplicates(subset=["id"], keep="last")

    upsert_df("targets", t[["id","device_id","name","input_type","unit","lower","upper","ord"]], ["id"])


    # targets
    t = df[["building_name","location_name","floor_name","room_name","device_name",
            "target_id","target_name","target_type_id","unit","lower","upper","order_no"]].copy()
    t["device_id"] = t.apply(lambda r: _dev_id(r["building_name"],r["location_name"],
                                               r["floor_name"],r["room_name"],r["device_name"]), axis=1)
    t = t.rename(columns={"target_id":"id","target_name":"name",
                          "target_type_id":"input_type","order_no":"ord"})
    upsert_df("targets", t[["id","device_id","name","input_type","unit","lower","upper","ord"]].drop_duplicates(), ["id"])

    # schedules / schedule_targets ã¯æ—¢å­˜å‡¦ç†ã®ã¾ã¾ã§OK
    st.success(f"ãƒã‚¹ã‚¿å–è¾¼: {len(df)} è¡Œï¼ˆä¸è¶³åˆ—ã¯æ—¢å®šå€¤ã§è£œå®Œï¼‰")

def import_annual_plan(df: pd.DataFrame):
    df = df.copy()
    df.columns = [str(c).strip().lower().lstrip("\ufeff") for c in df.columns]

    # åˆ—åã‚¨ã‚¤ãƒªã‚¢ã‚¹ï¼ˆæ—¥æœ¬èª/è‹±èªã©ã¡ã‚‰ã§ã‚‚OKï¼‰
    alias = {
        "schedule_id": ["schedule_id","ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«id","ãƒã‚±ãƒƒãƒˆid","job_id","æ¥­å‹™id","id"],
        "name":        ["schedule_name","job_name","æ¥­å‹™å","ä½œæ¥­å","åç§°","name"],
        "freq":        ["freq","frequency","å‘¨æœŸ","é »åº¦"],
        # ã©ã¡ã‚‰ã‹ã®å½¢ã§æ¥ã‚‹æƒ³å®šï¼ˆåˆ—ã”ã¨ or ä¸€è¦§ï¼‰
        "date":        ["date","äºˆå®šæ—¥","å®Ÿæ–½æ—¥","æ—¥ä»˜"],
        "start":       ["start","é–‹å§‹æ—¥","from"],
        "end":         ["end","çµ‚äº†æ—¥","to"],
        "status":      ["status","ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹","çŠ¶æ…‹"]
    }
    for canon, alts in alias.items():
        if canon not in df.columns:
            for a in alts:
                if a in df.columns:
                    df.rename(columns={a: canon}, inplace=True)
                    break

    # schedulesï¼ˆãƒ¦ãƒ‹ãƒ¼ã‚¯ï¼‰ã‚’ upsert
    sch = df[["schedule_id","name","freq"]].dropna(subset=["schedule_id"]).copy()
    sch["schedule_id"] = sch["schedule_id"].astype(str).str.strip()
    sch["name"] = sch["name"].astype(str).str.strip()
    sch = sch.drop_duplicates(subset=["schedule_id"])
    sch = sch.rename(columns={"schedule_id":"id"})
    upsert_df("schedules", sch[["id","name","freq"]], ["id"])

    # äºˆå®šæ—¥ã®ä½œæˆ
    dates = pd.DataFrame(columns=["schedule_id","date","status","done","total","done_at"])
    if "date" in df.columns:
        # è¡Œã”ã¨ã«äºˆå®šæ—¥ãŒå…¥ã£ã¦ã„ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³
        dates = pd.DataFrame({
            "schedule_id": df["schedule_id"].astype(str).str.strip(),
            "date": pd.to_datetime(df["date"], errors="coerce").dt.date,
            "status": df.get("status"),
            "done": None, "total": None, "done_at": None
        }).dropna(subset=["schedule_id","date"])
    elif {"start","end"}.issubset(df.columns):
        # æœŸé–“ + freq ã‹ã‚‰å±•é–‹ã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆfreqã¯ä»»æ„ï¼‰
        tmp = []
        for r in df.itertuples(index=False):
            sid = str(getattr(r, "schedule_id")).strip()
            if not sid: 
                continue
            start = pd.to_datetime(getattr(r, "start"), errors="coerce")
            end   = pd.to_datetime(getattr(r, "end"), errors="coerce")
            if pd.isna(start) or pd.isna(end) or end < start:
                continue
            # ç°¡æ˜“å±•é–‹ï¼šfreq ãŒ 'monthly','weekly','daily' ãªã©ã‚’æƒ³å®š
            freq = (str(getattr(r, "freq") or "")).lower()
            rule = {"monthly":"MS", "week":"W", "weekly":"W", "day":"D", "daily":"D"}.get(freq, "W")
            for d in pd.date_range(start=start, end=end, freq=rule):
                tmp.append((sid, d.date(), None))
        if tmp:
            dates = pd.DataFrame(tmp, columns=["schedule_id","date","status"])
            dates["done"]=None; dates["total"]=None; dates["done_at"]=None

    if not dates.empty:
        upsert_df("schedule_dates", dates, ["schedule_id","date"])
        st.success(f"å¹´é–“æ¥­å‹™è¨ˆç”» å–è¾¼: schedules {sch.shape[0]} ä»¶ / äºˆå®šæ—¥ {dates.shape[0]} ä»¶")
    else:
        st.success(f"å¹´é–“æ¥­å‹™è¨ˆç”» å–è¾¼: schedules {sch.shape[0]} ä»¶ï¼ˆäºˆå®šæ—¥ã¯åˆ—ãªã—ï¼‰")

# ---- å–è¾¼ï¼šé‹ç”¨ãƒã‚±ãƒƒãƒˆï¼ˆå®Ÿæ–½æ—¥ãƒ»é€²æ—ï¼‰ ---------------------------------
def import_tickets(df: pd.DataFrame):
    df = df.copy()
    df.columns = [str(c).strip().lower().lstrip("\ufeff") for c in df.columns]

    alias = {
        "date":        ["å®Ÿæ–½æ—¥","äºˆå®šæ—¥","æ—¥ä»˜","date"],
        "schedule_id": ["schedule_id","ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«id","ãƒã‚±ãƒƒãƒˆid"],
        "job_id":      ["job_id","æ¥­å‹™id","æ¥­å‹™ã‚³ãƒ¼ãƒ‰"],
        "name":        ["job_name","æ¥­å‹™å","ä½œæ¥­å","name"],
        "status":      ["status","ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹","çŠ¶æ…‹"],
        "progress":    ["progress","é€²æ—","é”æˆ","å®Œäº†/ç·æ•°"]
    }
    for canon, alts in alias.items():
        if canon not in df.columns:
            for a in alts:
                if a in df.columns:
                    df.rename(columns={a: canon}, inplace=True)
                    break

    # schedule_id ãŒç„¡ã„å ´åˆã¯ job_id ã¾ãŸã¯ name ã‹ã‚‰å¼•ã
    if "schedule_id" not in df.columns or df["schedule_id"].isna().all():
        sch = con.execute("SELECT id AS schedule_id, name FROM schedules WHERE tenant=?", [TENANT]).df()
        df["schedule_id"] = None
        if "job_id" in df.columns:
            # job_id = schedules.id ã¨ä¸€è‡´ã™ã‚‹æƒ³å®š
            df.loc[df["schedule_id"].isna(), "schedule_id"] = df.loc[df["schedule_id"].isna(), "job_id"]
        if "name" in df.columns and not sch.empty:
            # name ã‚’ã‚­ãƒ¼ã«è§£æ±ºï¼ˆå‰å¾Œç©ºç™½ç„¡è¦–ï¼‰
            m = df["schedule_id"].isna()
            df.loc[m, "schedule_id"] = df.loc[m, "name"].astype(str).str.strip().map(
                sch.set_index(sch["name"].astype(str).str.strip())["schedule_id"]
            )

    need = ["date","schedule_id","status"]
    miss = [c for c in need if c not in df.columns]
    if miss:
        st.error(f"å¿…é ˆåˆ—ãŒä¸è¶³: {miss}")
        st.write("å—ã‘å–ã£ãŸåˆ—:", list(df.columns))
        return

    s = pd.DataFrame()
    s["schedule_id"] = df["schedule_id"].astype(str).str.strip()
    s["date"]        = pd.to_datetime(df["date"], errors="coerce").dt.date
    s["status"]      = df["status"].astype(str).str.strip()

    # é€²æ—ï¼ˆ59/61 ç­‰ï¼‰ã‚’åˆ†è§£
    if "done" in df.columns and "total" in df.columns:
        s["done"]  = pd.to_numeric(df["done"], errors="coerce")
        s["total"] = pd.to_numeric(df["total"], errors="coerce")
    else:
        prog = df.get("progress")
        if prog is not None and len(df)>0:
            def split_prog(x):
                xs = str(x).replace(" ", "")
                if "/" in xs:
                    a,b = xs.split("/",1)
                    try:    return int(a or 0), int(b or 0)
                    except: return None, None
                return None, None
            d,t = zip(*[split_prog(v) for v in prog])
            s["done"], s["total"] = d, t

    s["done_at"] = pd.to_datetime(df.get("done_at") or df.get("å®Œäº†æ—¥æ™‚"), errors="coerce")

    # schedule_id ã¨ date ã®æ¬ æ/ç©ºã‚’é™¤å¤–
    s = s.dropna(subset=["schedule_id","date"])
    s = s[s["schedule_id"].astype(str).str.len() > 0]

    upsert_df("schedule_dates", s, ["schedule_id","date"])
    st.success(f"ãƒã‚±ãƒƒãƒˆå–è¾¼: {len(s)} è¡Œï¼ˆschedule_id è‡ªå‹•è§£æ±ºã‚ã‚Šï¼‰")


# ---- å–è¾¼ï¼šä¸å…·åˆï¼ˆéšå±¤ãƒ’ãƒ³ãƒˆã§ device_id è‡ªå‹•è§£æ±ºï¼‰ --------------------
def import_issues(df: pd.DataFrame):
    df = df.rename(columns=str.lower)

    # reported_on / due_on ã®åˆ¥åã«å¯¾å¿œ
    if "reported_on" not in df.columns and "ç™ºç”Ÿæ—¥æ™‚" in df.columns:
        df.rename(columns={"ç™ºç”Ÿæ—¥æ™‚":"reported_on"}, inplace=True)
    if "due_on" not in df.columns and "å¯¾å¿œæœŸé™" in df.columns:
        df.rename(columns={"å¯¾å¿œæœŸé™":"due_on"}, inplace=True)

    # è¨­å‚™åãªã©ã®ãƒ’ãƒ³ãƒˆåˆ—ã‚’å–ã‚Šè¾¼ã‚€
    if "è¨­å‚™" in df.columns and "device_id" not in df.columns:
        df["_equip_hint"] = df["è¨­å‚™"]
    elif "device" in df.columns and "device_id" not in df.columns:
        df["_equip_hint"] = df["device"]
    elif "equipment" in df.columns and "device_id" not in df.columns:
        df["_equip_hint"] = df["equipment"]

    for col in [("éƒ¨å±‹å","_room_hint"),("room_name","_room_hint"),
                ("ãƒ•ãƒ­ã‚¢","_floor_hint"),("floor_name","_floor_hint"),
                ("æ£Ÿ","_loc_hint"),("location_name","_loc_hint"),
                ("ç‰©ä»¶","_bld_hint"),("building_name","_bld_hint")]:
        if col[0] in df.columns: df[col[1]] = df[col[0]]

    # devices ä¸€è¦§ï¼ˆéšå±¤åˆ—ã‚ã‚Šï¼‰
    devs = con.execute("SELECT id,name,building_id,location_id,floor_id,room_id FROM devices WHERE tenant=?",
                       [TENANT]).df()

    def resolve_device(row):
        cand = devs.copy()
        if pd.notna(row.get("_equip_hint")): cand = cand[cand["name"]==row["_equip_hint"]]
        if pd.notna(row.get("_room_hint")):  cand = cand[cand["room_id"]==row["_room_hint"]]
        if pd.notna(row.get("_floor_hint")): cand = cand[cand["floor_id"]==row["_floor_hint"]]
        if pd.notna(row.get("_loc_hint")):   cand = cand[cand["location_id"]==row["_loc_hint"]]
        if pd.notna(row.get("_bld_hint")):   cand = cand[cand["building_id"]==row["_bld_hint"]]
        if len(cand)==1:
            return cand.iloc[0]["id"]
        cand2 = devs[devs["name"]==row.get("_equip_hint")]
        return cand2.iloc[0]["id"] if len(cand2)==1 else None

    if "device_id" not in df.columns:
        df["device_id"] = df.apply(resolve_device, axis=1)

    # æ­£è¦åŒ–ã—ã¦ä¿å­˜
    df["reported_on"] = pd.to_datetime(df.get("reported_on"), errors="coerce").dt.date
    if "due_on" in df.columns: df["due_on"] = pd.to_datetime(df["due_on"], errors="coerce").dt.date
    keep = ["id","device_id","reported_on","due_on","status","severity","category","summary","cmms_url_rule"]
    for k in keep:
        if k not in df.columns: df[k]=None
    out = df[keep].dropna(subset=["id","reported_on"])
    upsert_df("issues", out, ["id"])
    st.success(f"ä¸å…·åˆå–è¾¼: {len(out)} è¡Œï¼ˆdevice_idè‡ªå‹•è§£æ±º {out['device_id'].notna().sum()}ä»¶ï¼‰")

# ---- å–è¾¼ï¼šç‚¹æ¤œçµæœï¼ˆæ¨ªæŒã¡â†’ç¸¦meltï¼‰ -----------------------------------
def import_results_wide(df: pd.DataFrame):
    df = df.rename(columns=lambda c: str(c).strip())
    df = df.rename(columns=str.lower)

    # ãƒ˜ãƒƒãƒ€ãƒ¼ãŒæ—¥ä»˜å½¢å¼ã®ã‚‚ã®ã ã‘ï¼ˆYYYY-MM-DD / YYYY/MM/DDï¼‰
    date_cols = [c for c in df.columns
                 if re.fullmatch(r"\d{4}[-/]\d{2}[-/]\d{2}", c)
                 or re.fullmatch(r"\d{4}/\d{2}/\d{2}", c)]

    rows = []
    for _, r in df.iterrows():
        for dc in date_cols:
            val = r.get(dc)
            if pd.isna(val) or str(val) == "":
                continue
            d = pd.to_datetime(dc.replace("/", "-"), errors="coerce")
            if pd.isna(d):
                continue
            num = pd.to_numeric(val, errors="coerce")
            rows.append({
                "schedule_id": r.get("schedule_id"),
                "date": d.date(),
                "target_id": r.get("target_id"),
                "value_num": None if pd.isna(num) else float(num),
                "value_text": None if not pd.isna(num) else str(val),
                "value_bool": None,
                "judged": None
            })

    out = pd.DataFrame(rows)
    out = out.dropna(subset=["schedule_id","target_id","date"])
    if not out.empty:
        upsert_df("results", out, ["schedule_id","date","target_id"])
    st.success(f"ç‚¹æ¤œçµæœå–è¾¼: {len(df)} è¡Œï¼ˆ{len(date_cols)}æœ¬ã®æ—¥ä»˜åˆ—ã‚’å‡¦ç†ã€ãƒ¬ã‚³ãƒ¼ãƒ‰{len(out)}ä»¶ï¼‰")

# ---- KPI å†è¨ˆç®—ï¼ˆè¶…ç°¡æ˜“ï¼‰ ------------------------------------------------
def recalc_daily_kpis():
    con.execute("DELETE FROM daily_kpis WHERE tenant = ?", [TENANT])
    con.execute("""
        INSERT INTO daily_kpis
        SELECT ?, sd.date,
               COUNT(*) AS planned,
               SUM(CASE WHEN COALESCE(status,'') IN ('å®Œäº†','å®Ÿæ–½æ¸ˆ') THEN 1 ELSE 0 END) AS done,
               SUM(CASE WHEN sd.date < today() AND COALESCE(status,'') NOT IN ('å®Œäº†','å®Ÿæ–½æ¸ˆ') THEN 1 ELSE 0 END) AS overdue,
               0 AS findings
        FROM schedule_dates sd WHERE tenant=? GROUP BY sd.date
    """, [TENANT, TENANT])

# ========================= UI =============================================
def render_analysis():
  st.title("åˆ†æãƒ»ã‚µãƒãƒªãƒ¼ï¼ˆÎ²ç‰ˆï¼‰")
  tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs(["ğŸ“… æ—¥å ±","ğŸ› ï¸ è¨­å‚™","ğŸ“ˆ æœˆå ±","ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ","ğŸ“¥ å–è¾¼","ğŸ¤– AIÎ²"])
  
  # å–è¾¼
with tab5:
    st.subheader("CSV å–è¾¼")
    st.caption("æ¨å¥¨é †åºï¼šâ‘ ãƒã‚¹ã‚¿ â†’ â‘¡å¹´é–“æ¥­å‹™è¨ˆç”» â†’ â‘¢operation_tickets â†’ â‘£issues â†’ â‘¤ç‚¹æ¤œçµæœ â†’ KPIå†è¨ˆç®—")

    # â‘  ãƒã‚¹ã‚¿ï¼ˆéšå±¤+è¨­å‚™+targetsï¼‰â€” æ—¢å­˜ã®ã¾ã¾
    f_master = st.file_uploader("ãƒã‚¹ã‚¿ï¼ˆéšå±¤+è¨­å‚™+targetï¼‰CSV", type=["csv"], key="upl_master")
    if f_master:
        import_master(pd.read_csv(f_master, encoding="utf-8-sig"))

    # â‘¡ å¹´é–“æ¥­å‹™è¨ˆç”»ï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®šç¾©ï¼äºˆå®šæ—¥ï¼‰
    f_plan = st.file_uploader("å¹´é–“æ¥­å‹™è¨ˆç”».csvï¼ˆã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å®šç¾©/äºˆå®šæ—¥ï¼‰", type=["csv"], key="upl_plan")
    if f_plan:
        import_annual_plan(pd.read_csv(f_plan, encoding="utf-8-sig"))

    # â‘¢ å®Ÿæ–½ãƒã‚±ãƒƒãƒˆï¼ˆå®Ÿæ–½æ—¥/é€²æ—ï¼‰â€” schedule_id ãŒç„¡ãã¦ã‚‚ job_id/æ¥­å‹™åã‹ã‚‰è‡ªå‹•è§£æ±º
    f_tickets = st.file_uploader("operation_tickets.csvï¼ˆå®Ÿæ–½æ—¥/é€²æ—ï¼šscheduleè‡ªå‹•è§£æ±ºå¯ï¼‰", type=["csv"], key="upl_tickets")
    if f_tickets:
        import_tickets(pd.read_csv(f_tickets, encoding="utf-8-sig"))

    # â‘£ ä¸å…·åˆ
    f_issues = st.file_uploader("issues.csvï¼ˆä¸å…·åˆï¼‰", type=["csv"], key="upl_issues")
    if f_issues:
        import_issues(pd.read_csv(f_issues, encoding="utf-8-sig"))

    # â‘¤ ç‚¹æ¤œçµæœï¼ˆæ¨ªæŒã¡ï¼‰
    f_results = st.file_uploader("ç‚¹æ¤œçµæœï¼ˆæ¨ªæŒã¡ï¼‰CSV", type=["csv"], key="upl_results")
    if f_results:
        import_results_wide(pd.read_csv(f_results, encoding="utf-8-sig"))

    # KPI å†è¨ˆç®—
    if st.button("KPIå†è¨ˆç®—"):
        recalc_daily_kpis()
        st.success("daily_kpis å†è¨ˆç®—")

  
  # æ—¥å ±
  with tab1:
      st.subheader("æœ¬æ—¥ã®æ¥­å‹™äºˆå®šã¨é€²æ—")
      target_date = st.date_input("å¯¾è±¡æ—¥", value=date.today())
      kpi = con.execute(
          "SELECT planned, done, overdue FROM daily_kpis WHERE tenant=? AND date=?",
          [TENANT, target_date]).fetchone()
      planned, done, overdue = (kpi if kpi else (0,0,0))
      c1,c2,c3 = st.columns(3); c1.metric("äºˆå®š", planned); c2.metric("å®Œäº†", done); c3.metric("æœŸé™è¶…é", overdue)
      df = con.execute("SELECT * FROM schedule_dates WHERE tenant=? AND date=?", [TENANT, target_date]).df()
      st.dataframe(df, use_container_width=True)
  
      st.subheader("æœ¬æ—¥ç™ºç”Ÿã®ä¸å…·åˆ")
      issues = con.execute("SELECT * FROM issues WHERE tenant=? AND reported_on=?", [TENANT, target_date]).df()
      st.dataframe(issues, use_container_width=True)
  
  # è¨­å‚™
  with tab2:
      st.title("ğŸ› ï¸ è¨­å‚™ãƒšãƒ¼ã‚¸")
  
      # æ®µéšé¸æŠï¼ˆç‰©ä»¶â†’æ£Ÿâ†’ãƒ•ãƒ­ã‚¢â†’éƒ¨å±‹â†’è¨­å‚™ï¼‰
      blds = con.execute("SELECT DISTINCT building_id FROM devices WHERE tenant=?", [TENANT]).df()["building_id"].tolist()
      if not blds:
          st.info("ã¾ãšã¯ã€ğŸ“¥ å–è¾¼ã€ã‹ã‚‰ãƒã‚¹ã‚¿CSVã‚’æŠ•å…¥ã—ã¦ãã ã•ã„ã€‚"); st.stop()
      bld = st.selectbox("ç‰©ä»¶", blds)
  
      locs = con.execute(
          "SELECT DISTINCT location_id FROM devices WHERE tenant=? AND building_id=?",
          [TENANT, bld]).df()["location_id"].tolist()
      loc = st.selectbox("æ£Ÿ", locs) if locs else None
  
      flrs = con.execute(
          "SELECT DISTINCT floor_id FROM devices WHERE tenant=? AND building_id=? AND location_id=?",
          [TENANT, bld, loc]).df()["floor_id"].tolist() if loc else []
      flr = st.selectbox("ãƒ•ãƒ­ã‚¢", flrs) if flrs else None
  
      rooms = con.execute(
          "SELECT DISTINCT room_id FROM devices WHERE tenant=? AND building_id=? AND location_id=? AND floor_id=?",
          [TENANT, bld, loc, flr]).df()["room_id"].tolist() if flr else []
      room = st.selectbox("éƒ¨å±‹", rooms) if rooms else None
  
      devs = con.execute(
          """SELECT id,name FROM devices
             WHERE tenant=? AND building_id=? AND location_id=? AND floor_id=? AND room_id=?""",
          [TENANT, bld, loc, flr, room]).df() if room else pd.DataFrame(columns=["id","name"])
      dev = st.selectbox("è¨­å‚™", options=devs["id"] if not devs.empty else [])
      if not dev:
          st.stop()
  
      meta = con.execute("SELECT * FROM devices WHERE tenant=? AND id=?", [TENANT, dev]).df().iloc[0]
      st.markdown(f"### {meta['name']}ã€€ã€”{bld} / {loc} / {flr} / {room}ã€•")
      if pd.notna(meta.get("cmms_url_rule")) and str(meta.get("cmms_url_rule")) not in ("", "None"):
          st.link_button("CMMSã§é–‹ã", str(meta["cmms_url_rule"]))
  
      # æœŸé–“
      dr = st.date_input("æœŸé–“", [])
      if len(dr) != 2:
          st.info("æœŸé–“ã‚’é¸æŠã™ã‚‹ã¨ã€ã‚°ãƒ©ãƒ•ã¨è¡¨ãŒè¡¨ç¤ºã•ã‚Œã¾ã™ã€‚"); st.stop()
  
      # ç‚¹æ¤œçµæœï¼ˆtargets ã¨é–¾å€¤ã‚’ JOINï¼‰
      q = """
        SELECT r.date, r.target_id, r.value_num, r.value_text,
               t.name AS target_name, t.unit, t.lower, t.upper
        FROM results r
        JOIN targets t ON t.tenant=r.tenant AND t.id=r.target_id
        WHERE r.tenant=? AND t.device_id=? AND r.date BETWEEN ? AND ?
        ORDER BY r.date
      """
      df = con.execute(q, [TENANT, dev, dr[0], dr[1]]).df()
      if df.empty:
          st.warning("æœŸé–“å†…ã®ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚"); st.stop()
  
      df["date"] = pd.to_datetime(df["date"])
      # ç•°å¸¸ï¼ˆé–¾å€¤é€¸è„± or Ã—/NGï¼‰
      abnormal_mask = (
          (df["value_num"].notna() & (
              (df["lower"].notna() & (df["value_num"] < df["lower"])) |
              (df["upper"].notna() & (df["value_num"] > df["upper"]))
          )) |
          (df["value_num"].isna() & df["value_text"].isin(["Ã—","NG"]))
      )
      c1, c2, c3 = st.columns(3)
      c1.metric("å¯¾è±¡é …ç›®", int(df["target_id"].nunique()))
      c2.metric("ãƒ‡ãƒ¼ã‚¿ç‚¹æ•°", int(df.shape[0]))
      c3.metric("ç•°å¸¸ä»¶æ•°", int(abnormal_mask.sum()))
  
      # äº”æ„Ÿã®ã‚¹ã‚³ã‚¢åŒ–
      label_to_score = {"â—‹":0, "OK":0, "è‰¯":0, "â–³":1, "è¦ç¢ºèª":1, "æ³¨æ„":1, "Ã—":2, "NG":2, "ç•°å¸¸":2}
      num  = df.dropna(subset=["value_num"]).copy()
      qual = df[df["value_num"].isna()].copy()
      qual["score"] = qual["value_text"].map(label_to_score).fillna(np.nan)
  
      # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ç”¨ãƒ‡ãƒ¼ã‚¿
      if not qual.empty:
          heat = (qual.pivot_table(index="target_name", columns="date", values="score", aggfunc="max")
                  .sort_index())
      else:
          heat = pd.DataFrame(index=[], columns=[])
  
      # åŒä¸€æ™‚é–“è»¸ã®è¤‡åˆå›³
     
    # ---- æ•°å€¤ãƒ©ã‚¤ãƒ³ + Ã—/â–³ èƒŒæ™¯ã‚·ã‚§ãƒ¼ãƒ‰ï¼ˆåŒä¸€æ™‚é–“è»¸ãƒ»å³è»¸ã‚’UIã§é¸æŠï¼‰ ----
      df["date"] = pd.to_datetime(df["date"], errors="coerce")
      num  = df.dropna(subset=["value_num"]).copy()
      qual = df[df["value_num"].isna()].copy()
      
      # äº”æ„Ÿâ†’sev(0=OK,1=â–³,2=Ã—)
      sev_map = {"â—‹":0, "OK":0, "è‰¯":0, "â–³":1, "è¦ç¢ºèª":1, "æ³¨æ„":1, "â–²":1, "Ã—":2, "NG":2, "ç•°å¸¸":2}
      if "sev" not in qual.columns:
          qual["sev"] = qual["value_text"].map(sev_map).astype("float")
      qual["date"] = pd.to_datetime(qual["date"], errors="coerce")
      
      # èƒŒæ™¯ã‚·ã‚§ãƒ¼ãƒ‰å¯¾è±¡ã®é¸æŠ
      qual_targets_all = sorted(qual["target_name"].dropna().unique().tolist())
      default_sel = sorted(qual.loc[qual["sev"]==2, "target_name"].dropna().unique().tolist()) or qual_targets_all
      sel_targets = st.multiselect("èƒŒæ™¯è‰²ã®å¯¾è±¡ï¼ˆäº”æ„Ÿé …ç›®ï¼‰", qual_targets_all, default=default_sel)
      shade_x     = st.checkbox("Ã—ã®ç™ºç”Ÿæ—¥ã§èƒŒæ™¯è‰²", value=True)
      shade_tri   = st.checkbox("â–³ã®ç™ºç”Ÿæ—¥ã‚‚èƒŒæ™¯è‰²ã«å«ã‚ã‚‹", value=False)
      shade_alpha = st.slider("èƒŒæ™¯ã®æ¿ƒã•", 0.05, 0.4, 0.12, 0.01)
      
      qual_sel  = qual[qual["target_name"].isin(sel_targets)] if len(sel_targets) else qual.iloc[0:0]
      bad_days  = pd.to_datetime(qual_sel.loc[qual_sel["sev"]>=2, "date"], errors="coerce").dt.normalize().dropna().unique() if shade_x else []
      warn_days = pd.to_datetime(qual_sel.loc[qual_sel["sev"]==1, "date"], errors="coerce").dt.normalize().dropna().unique() if shade_tri else []
      
      # å³è»¸ã«å‡ºã™ã‚·ãƒªãƒ¼ã‚ºã‚’é¸æŠï¼ˆå˜ä½æ¬ æã§ã‚‚OKï¼‰
      series = (num.groupby("target_name")
                  .agg(unit=("unit", lambda s: next((u for u in s if pd.notna(u) and str(u) not in ["nan","None",""]), "")))
                  .reset_index())
      
      KW_RIGHT = ("åœ§åŠ›","å·®åœ§","åœ§","MPa","kPa","é›»åœ§","V")  # åˆæœŸå€™è£œ
      suggest_right = [row.target_name for row in series.itertuples(index=False)
                       if any(k in row.target_name for k in KW_RIGHT)]
      right_targets = st.multiselect("å³è»¸ã«ã™ã‚‹é …ç›®ï¼ˆåœ§åŠ›ãƒ»é›»åœ§ãªã©ï¼‰",
                                     series["target_name"].tolist(), default=suggest_right)
      
      from plotly.subplots import make_subplots
      import plotly.graph_objects as go
      fig = make_subplots(specs=[[{"secondary_y": True}]])
      
      # æ•°å€¤ãƒˆãƒ¬ãƒ¼ã‚¹
      for tname, g in num.groupby("target_name"):
          unit_label = series.loc[series["target_name"]==tname, "unit"].iloc[0]
          fig.add_trace(
              go.Scatter(x=g["date"], y=g["value_num"], mode="lines+markers",
                         name=tname, hovertemplate="%{x|%Y-%m-%d}<br>%{y} "+(unit_label or "")),
              secondary_y=(tname in right_targets)
          )
      
      # èƒŒæ™¯ã‚·ã‚§ãƒ¼ãƒ‰
      for d in warn_days:
          fig.add_vrect(x0=d, x1=d, fillcolor="#FFD166", opacity=shade_alpha, line_width=0)
      for d in bad_days:
          fig.add_vrect(x0=d, x1=d, fillcolor="#EF476F", opacity=min(shade_alpha+0.05, 0.5), line_width=0)
      
      # ä¸Šéƒ¨ãƒãƒ¼ã‚«ãƒ¼
      if st.checkbox("Ã—/â–³ ã‚’ä¸Šéƒ¨ã«è¨˜å·è¡¨ç¤º", value=True):
          for d in bad_days:
              fig.add_annotation(x=d, y=1.02, xref="x", yref="paper", text="Ã—", showarrow=False,
                                 font=dict(color="#EF476F", size=14))
          for d in warn_days:
              fig.add_annotation(x=d, y=1.02, xref="x", yref="paper", text="â–²", showarrow=False,
                                 font=dict(color="#FFD166", size=12))
      
      # è»¸ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆâ†ã‚¹ã‚¯ã‚·ãƒ§ã§ left_label ãŒã€Œå³è»¸ã€ã«ãªã£ã¦ãŸã®ã§ç›´ã—ã¦ã­ï¼‰
      left_units  = series.loc[~series["target_name"].isin(right_targets), "unit"].unique().tolist()
      right_units = series.loc[ series["target_name"].isin(right_targets), "unit"].unique().tolist()
      left_label  = " / ".join([u for u in left_units  if u]) or "å·¦è»¸"
      right_label = " / ".join([u for u in right_units if u]) or "å³è»¸"
      fig.update_yaxes(title_text=left_label,  secondary_y=False)
      fig.update_yaxes(title_text=right_label, secondary_y=True)
      
      fig.update_layout(height=520, margin=dict(l=10,r=10,t=30,b=10),
                        legend=dict(orientation="h", yanchor="bottom", y=1.02, x=0))
      st.plotly_chart(fig, use_container_width=True)
  
      
      # --- ä¸‹ï¼šã‚¤ãƒ™ãƒ³ãƒˆï¼ˆäº”æ„Ÿï¼‰ä¸€è¦§ ---
      if not qual.empty:
          show = qual_sel.copy() if len(sel_targets) else qual.copy()
          col1, col2 = st.columns(2)
          f_bad = col1.checkbox("Ã—ã®ã¿è¡¨ç¤º", value=False)
          f_tri = col2.checkbox("â–³ã®ã¿è¡¨ç¤º", value=False)
          if f_bad: show = show[show["sev"]==2]
          if f_tri: show = show[show["sev"]==1]
          st.dataframe(
              show.sort_values(["date","target_name"])[["date","target_name","value_text"]]
                  .rename(columns={"date":"æ—¥ä»˜","target_name":"é …ç›®","value_text":"åˆ¤å®š"}),
              use_container_width=True
          )
  
  
      # ä¸Šæ®µï¼šæ•°å€¤ãƒ©ã‚¤ãƒ³
      for name, g in num.groupby("target_name"):
          u = str(g["unit"].iloc[0] if "unit" in g else "")
          fig.add_trace(
              go.Scatter(x=g["date"], y=g["value_num"], mode="lines+markers",
                         name=str(name), hovertemplate="%{x|%Y-%m-%d}<br>%{y} "+u),
              row=1, col=1
          )
      # ç•°å¸¸æ—¥ã®ç¸¦å¸¯
      # äº”æ„Ÿã‚¹ã‚³ã‚¢åˆ—ã‚’ç”¨æ„ï¼ˆãªã‘ã‚Œã°ä½œæˆï¼‰
      if "sev" not in qual.columns:
          sev_map = {"â—‹":0, "OK":0, "è‰¯":0, "â–³":1, "è¦ç¢ºèª":1, "æ³¨æ„":1, "â–²":1, "Ã—":2, "NG":2, "ç•°å¸¸":2}
          qual["sev"] = qual["value_text"].map(sev_map).astype("float")
      
      # ç•°å¸¸æ—¥ã®ç¸¦å¸¯ï¼ˆÃ—/NGãªã© sev>=2ï¼‰
      bad_days = (
          pd.to_datetime(qual.loc[qual["sev"] >= 2, "date"], errors="coerce")
            .dt.normalize().dropna().unique()
      )
      
      warn_days = (
          pd.to_datetime(qual.loc[qual["sev"] == 1, "date"], errors="coerce")
            .dt.normalize().dropna().unique()
      )
  for d in warn_days:
      fig.add_vrect(x0=d, x1=d, row="all", col=1, fillcolor="#FFD166", opacity=0.10, line_width=0)
  
  for d in bad_days:
      fig.add_vrect(x0=d, x1=d, row="all", col=1, fillcolor="red", opacity=0.08, line_width=0)
      # ä¸‹æ®µï¼šäº”æ„Ÿãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
      if not heat.empty:
          colorscale = [[0.0, "#3CB371"], [0.5, "#FFD166"], [1.0, "#EF476F"]]  # 0=ç·‘,1=é»„,2=èµ¤
          fig.add_trace(
              go.Heatmap(z=heat.values, x=heat.columns, y=heat.index,
                         zmin=0, zmax=2, colorscale=colorscale, colorbar=dict(title=""),
                         hovertemplate="%{y}<br>%{x|%Y-%m-%d}<br>çŠ¶æ…‹=%{z}<extra></extra>"),
              row=2, col=1
          )
      fig.update_layout(height=700, margin=dict(l=10,r=10,t=40,b=10))
      fig.update_xaxes(matches="x", row=1, col=1)
      st.plotly_chart(fig, use_container_width=True)
  
      # æ•°å€¤è¦ç´„è¡¨
      if not num.empty:
          latest = num.sort_values("date").groupby("target_name").tail(1).set_index("target_name")[["value_num","unit"]]
          stats = num.groupby("target_name")["value_num"].agg(æœ€å°="min", æœ€å¤§="max", å¹³å‡="mean")
          summary = latest.join(stats, how="left").rename(columns={"value_num":"æœ€æ–°å€¤"})
          st.markdown("**æ•°å€¤ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®è¦ç´„**")
          st.dataframe(summary.reset_index(), use_container_width=True)
  
      # äº”æ„Ÿãƒ†ãƒ¼ãƒ–ãƒ«
      if not qual.empty:
          st.markdown("**äº”æ„Ÿ/é¸æŠã‚¿ãƒ¼ã‚²ãƒƒãƒˆï¼ˆè¡¨ï¼‰**")
          st.dataframe(
              qual.pivot_table(index="date", columns="target_name", values="value_text", aggfunc="first"),
              use_container_width=True
          )
  
      # ã“ã®è¨­å‚™ã®ä¸å…·åˆ
      st.subheader("ã“ã®è¨­å‚™ã«ç´ã¥ãä¸å…·åˆ")
      iss = con.execute(
          """SELECT id, reported_on, due_on, status, severity, category, summary
             FROM issues WHERE tenant=? AND device_id=?
             ORDER BY COALESCE(due_on, reported_on) DESC""",
          [TENANT, dev]).df()
      c1, c2, c3 = st.columns(3)
      if iss.empty:
          c1.metric("æœªå®Œäº†", 0); c2.metric("æœŸé™è¶…é", 0); c3.metric("ä»Šæœˆæ–°è¦", 0)
          st.info("ç´ã¥ãä¸å…·åˆã¯ã‚ã‚Šã¾ã›ã‚“ã€‚")
      else:
          not_done = ~iss["status"].isin(["å®Œäº†","å¯¾å¿œæ¸ˆ"])
          overdue = pd.to_datetime(iss["due_on"], errors="coerce") < pd.Timestamp.today()
          this_month = pd.to_datetime(iss["reported_on"], errors="coerce").dt.to_period("M") == pd.Timestamp.today().to_period("M")
          c1.metric("æœªå®Œäº†", int(iss[not_done].shape[0]))
          c2.metric("æœŸé™è¶…é", int(iss[not_done & overdue].shape[0]))
          c3.metric("ä»Šæœˆæ–°è¦", int(iss[this_month].shape[0]))
          st.dataframe(iss, use_container_width=True)
  
      # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ
      st.subheader("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
      docs = con.execute(
          """SELECT d.id, d.title, d.category, d.tags, d.ai_summary
             FROM document_bindings b
             JOIN documents d ON d.tenant=b.tenant AND d.id=b.doc_id
             WHERE b.tenant=? AND b.entity_type='device' AND b.entity_id=?
             ORDER BY d.uploaded_at DESC""",
          [TENANT, dev]).df()
      if docs.empty:
          st.info("ã“ã®è¨­å‚™ã«ç´ã¥ããƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã¯æœªç™»éŒ²ã§ã™ã€‚")
      else:
          st.dataframe(docs, use_container_width=True)
  
      # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå®šç¾©
      st.subheader("ç‚¹æ¤œé …ç›®ï¼ˆTargets å®šç¾©ï¼‰")
      tl = con.execute(
          """SELECT id AS target_id, name, input_type, unit, lower, upper, ord
             FROM targets WHERE tenant=? AND device_id=? ORDER BY ord""",
          [TENANT, dev]).df()
      st.dataframe(tl, use_container_width=True)
  
  # æœˆå ±ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ï¼‰
  with tab3:
      st.subheader("æŒ‡å®šæœˆã®ã‚µãƒãƒªãƒ¼ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ï¼‰")
      st.info("CSVå–è¾¼å¾Œã€æœˆæ¬¡é›†è¨ˆã‚’å®Ÿè£…ã—ã¾ã™ã€‚")
  
  # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ï¼‰
  with tab4:
      st.subheader("ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆä¸€è¦§ï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ï¼‰")
      docs = con.execute("SELECT id,title,category,tags,ai_summary FROM documents WHERE tenant=?", [TENANT]).df()
      st.dataframe(docs, use_container_width=True)
  
  # AIï¼ˆãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ï¼‰
  with tab6:
      st.subheader("AIã‚µãƒãƒªãƒ¼ï¼ˆÎ²ï¼‰")
      st.info("OCR/AIé€£æºã¯å¾Œã§æ¥ç¶šã€‚ã¾ãšã¯CSVâ†’ãƒ€ãƒƒã‚·ãƒ¥ãƒœãƒ¼ãƒ‰ã®æµã‚Œã‚’å›ºã‚ã¾ã™ã€‚")
  
  st.caption("Theme: ç®¡ç†ãƒ­ã‚¤ãƒ‰é¢¨ / ãƒ‡ãƒ¼ã‚¿ã¯ UTF-8 CSV ã‚’ å–è¾¼ã‚¿ãƒ–ã‹ã‚‰æŠ•å…¥")

import streamlit as st
import streamlit.components.v1 as components

# ---- å¤–éƒ¨CMMSã®ãƒ™ãƒ¼ã‚¹URLï¼ˆé©å®œå¤‰æ›´ï¼‰----
CMMS_BASE = st.secrets.get("CMMS_BASE", "https://your.cmms.example")
CMMS_LINKS = {
    "æ¥­å‹™ãƒã‚±ãƒƒãƒˆ":   f"{CMMS_BASE}/tickets",
    "ä¸å…·åˆç®¡ç†":     f"{CMMS_BASE}/issues",
    "ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°":   f"{CMMS_BASE}/monitoring",
    "å ±å‘Šæ›¸":         f"{CMMS_BASE}/reports",
    "å†™çœŸå ±å‘Š":       f"{CMMS_BASE}/photos",
    "å°å¸³ç®¡ç†":       f"{CMMS_BASE}/ledger",
    "å¹´é–“æ¥­å‹™è¨ˆç”»":   f"{CMMS_BASE}/annual-plan",
}

def render_sidebar_and_route():
    with st.sidebar:
        st.markdown("### ç®¡ç†ãƒ­ã‚¤ãƒ‰")

        # å¤–éƒ¨ãƒªãƒ³ã‚¯ï¼ˆâ†ã“ã“ã‚’ for ã®ä¸­ã§1æ®µã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆï¼‰
        for label in ["æ¥­å‹™ãƒã‚±ãƒƒãƒˆ","ä¸å…·åˆç®¡ç†","ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°","å ±å‘Šæ›¸","å†™çœŸå ±å‘Š","å°å¸³ç®¡ç†","å¹´é–“æ¥­å‹™è¨ˆç”»"]:
            st.link_button(f"ãƒ»{label}", CMMS_LINKS[label], use_container_width=True, type="secondary")

        st.divider()

        # å¹´é–“æ¥­å‹™è¨ˆç”»ã®ç›´ä¸‹ã« Î²ç‰ˆï¼ˆã‚¢ãƒ—ãƒªå†…ãƒ«ãƒ¼ãƒˆï¼‰
        go_analysis = st.button("ãƒ»åˆ†æãƒ»ã‚µãƒãƒªãƒ¼ï¼ˆÎ²ç‰ˆï¼‰", type="primary",
                                use_container_width=True, key="btn_analysis")

    # ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ï¼ˆæŠ¼ã—ãŸã‚‰ä¿æŒï¼‰
    if go_analysis or st.session_state.get("route") == "analysis":
        st.session_state["route"] = "analysis"
        render_analysis()
    else:
        st.title("ç®¡ç†ãƒ­ã‚¤ãƒ‰ ã‚·ã‚§ãƒ«")
        st.info("å·¦ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‹ã‚‰å„ç”»é¢ã¸ã€‚ã€åˆ†æãƒ»ã‚µãƒãƒªãƒ¼ï¼ˆÎ²ç‰ˆï¼‰ã€ã®ã¿æœ¬ã‚¢ãƒ—ãƒªå†…ã§å‹•ä½œã—ã¾ã™ã€‚")


# ---- èµ·å‹• ----
render_sidebar_and_route()

